# MedNormalization
Репозиторий для разработке методов нормализации, или entity linking - соотнесения упоминаний из текстов с концептами из словарей и классификаторов. 
# 0 Подготовка первой нормальной версии корпуса для нормализации.
## Формат корпуса
Формат хранения корпуса такой: jsonlines файл, каждая строка это json со следующими полями: **meta**, **raw**, **sentences**, **objects**, **coreference**, **context**

В **meta** хранятся:
- **fileName** - имя файла (или имя документа + расширение)
- **annotator** - пользователь, чья разметка была использована.

В **raw** хранится текст отзыва с заголовками, ссылкой и тэгами (TITLE, TEXT, RATING, NOTE).

В **sentences** хранится список предложений, каждое предложение - список слов, представленных как словари с полями: **forma** - словоформа из текста, **start** - позиция начала слова в тексте, **end** - позиция конца слова.

В **objects** есть вложенное поле MedEntity, в котором лежит список упоминаний из корпуса. Каждое упоминание это словарь со следующими полями : 
- **spans** - список спанов, из которых состоит упоминание. Каждый спан - словарь с полями **begin**, **end** (позиция начала спана  в тексте и конец спана, соответственно).
- **xmiID** - ID упоминания в пределах документа
- **text** - текст упоминания, сложенный из её спанов**
- **MedEntityType** - основной тип сущности (Medication, Disease, ADR, Note)
- **MedType** - подтип Medication (поле может отсутствовать, если это не Medication)
- **DisType** - подтип Disease (поле может отсутствовать, если это не Disease)
- **MedFrom** - поле присутствует только если **MedType**==Drugname. Может иметь одно из двух значений: Domestic, Foreign. Распространяется ли препарат на территории россии (Domestic) или только за границей (Foreign)
- **MedMaker** - поле присутствует только если **MedType**==MedMaker. Может иметь одно из двух значений: Domestic, Foreign. Является ли производитель препарата отечественным (Domestic) или иностранным (Foreign)
- **Context** - к каким контекстам в отзыве относится упоминание
- **Normalization** - сущность из словаря, соответствующая этому упоминанию.

В **coreference** представлены кореферентные цепочки в следующем формате:
- поле **mentions** содержит список словарей упоминаний. Каждое упоминание - словарь с полями **startPos**, **endPos**, указывающими границы упоминания в тексте
- поле **clusters** содержит список цепочек (кластеров). Каждая цепочка это список индексов упоминаний из поля mentions, которые являются кореферентными
 
В **context** записаны контекстные цепочки. Тот же формат, что и **coreference**
## Поле Normalization
У некоторых сущностей может присутствовать поле **Normalization**. В нём может быть записана фраза из словаря или класс из классификатора, которая соответствует этому упоминанию. Упоминания следующих типов могут иметь поле **Normalization**: 
- Medication:Drugname и Medication:Drugclass приведены к категориями из класификатора АТХ. В поле Normalization записан код.
- Medication:SourceInfodrug - приведены к нескольким категориям, которые мы посчитали значимыми
- Medication:Drugform приведены к лекарственным формам, перечисленным в приказе минздрава РФ от 27.07.2016 N 538н "Об утверждении перечня наименований лекарственных форм лекарственных препаратов для медицинского применения"
- Disease:Diseasename приведены к категориям из классификатора МКБ-10
- Disease:Indication, ADR приведены к словарю MedDRA.

Наибольший интерес представляют как раз Indication и ADR. Но они на данный момент проходят проверку и доразметку для доп. корпуса. Drugform должно быть относительно просто, кроме некоторых случаев. SourceInfodrug - это скорее классификация нежели нормализация, так не уверен, что есть какое-то внимание на этом заострять. Diseasename, Drugname и Drugclass - тоже скорее классификация, но более интересная, там много разных категорий в классификаторах и соотнести термин с категорией может быть сложно.

### словари
Словари надо положить в Data/External и описать.
### значения зависящие от контекста
Одинаковые упоминания (с одинаковым текстом) могут иметь разные значения в зависимости от текста в котором они встречаются. Такое встречается редко, но возможно.
### альтернативные значения
Некоторые упоминания могут иметь несколько вариантов нормализации. Они записаны через вертикальную черту "|" в поле Normalization. По хорошему, при обучении и оценки моделей надо считать правильным любой из предложенных вариантов. Но сначала надо посчитать, сколько таких случаев, стоит ли на счёт них заморачиваться, или лучше поставить случайный из всех возможных, или наиболее популярный.
### уровни иерархии
Для Drugname, Drugclass, Diseasename разметчики старались проставлять категории как можно более низкого уровня, но иногда это было не возможно, поэтому могут встречаться и более высокие уровни иерархий. 

Для ADR и Indication разметчики использовали уровень LLT из MedDRA. Но интерес представляет на самом деле уровень PT. Поэтому при переводе таблиц разметчиков в корпус в описанном выше формате LLT должны приводиться к PT.

Для Drugform и SourceInfodrug никаких иерархий нет.

Кроме того, я читаю, что ошибки модели, когда она указала не верную категорию, но принадлежащую к той правильной категории более высокого уровня, должны учитываться менее строго. 
### отсутствующие значения
Некоторые упоминания могут иметь пустое поле Normalization. Это значит, что разметчикам не удалось подобрать подходящий термин или категорию. Таким надо игнорировать при обучении.
### прочие особенности корпуса
Что-то я сюда хотел написать...

## Версии разметок
Разметка будет взята из проекта med_NNreview (дамп 10.01.2022) (для 2799 отзывов) и med-reviews_1kADR (дамп 11.01.2022).

По нормализации для основного корпуса проведена проверка Drugname, Diseasename, Drugform, SourceInfoDrug. Требует доработки: Drugclass, ADR, Indication.

По дополнительному корпусу проведена разметка Drugname, Diseasename, Drugform, SourceInfoDrug, Drugclass. Разметка ADR и Indication в процессе. Рекомендуется проверка: Drugname, Diseasename, Drugform, SourceInfoDrug, Drugclass
## Разбивка на трейн тест
Будет ли кросс валидация?

Надо разбить так, чтоб были сущности (словарные), которые есть только в тесте, были те, которые слабо представлены в трейне, сильно представлены в трейне. Это усложняется если принимать во внимание иерархическую структуру словарей.
## Актуальные задачи:
0.1 дописать описание

0.2 проверить слегка разметку, че там разметчики наделали, нет ли каких-то моментов, требующих внимания. Это сделал для всего кроме адр и индикейшн. Были моменты, которые пришлось править (некорректно написанные коды, коды с которыми я не согласен, косая черта вместо прямой при альтернативных вариантах итп). объединил разметку, которая делалась для двух корпусов в одну таблицу. А так, ждём теперь АДР, Индикейшн. А, только я не проверял на то, всегда ли похожие на мой взгляд сущности имеют одинаковые классы, потому что мне попадались такие, где были разные (из разных корпусов просто были)

0.3 драгкласс надо дать на перепроверку разметчикам.

0.4 сохранить корпус в описанном формате. Это легко, я уже делал, просто перезапустить, только к PT приведение меддры добавить.

0.5 сформулировать и зафиксировать, как делаем разбивку на трейн, валид, тест.

0.6 как-то посчитать агримент, есть версии пересекающиеся немного от разных разметчиков и есть версии в завимисмости от времени.

0.7 Посчитать, сколько случаев, когда есть несколько альтернативных вариантов, можно ли их проигнорировать.
# 1 Подготовка репозитория
1.1 Создать репозиторий RDR_normalization

Папка Data
- External - здесь будут сложены словари, для нормализации
- Raw - здесь будут складироваться версии корпуса в формате jsonlines
- Interim - здесь будут лежать варианты корпуса с разбивкой на трейн тест и возможно в каком-то измененном виде, например под разные формализации задачи, не знаю пока. 

Разные реализации будут в отдельных папках в одном репозитории. 

1.2 Можно положить в этот же репозиторий папки EntityNormalization_Elastic, EntityNormalization_SiamNN, EntityNormalization_W2V, MetaDialogNastyaGlebMap, с прошлыми наработками по нормализации из соответствующих репозиториев. Но это имеет смысл делать только если пробовать запускать эти механизмы на этом корпусе. Прошлые точности по ним, кстати тут (https://docs.google.com/spreadsheets/d/1RDfygKFgIqcMqQ2U-NavZxU86gndQ8sESjFTUD8e4TQ/edit#gid=250959241 )

1.3 В issue постараться вспомнить, какие вопросы возникали по разметке и какие проблемы могут возникнуть, например фразы, нормализация которых понятна только в контексте

# 2 Обзор литературы и выбор метода
Здесь собираются корпуса и достигнутые на них точности. https://docs.google.com/spreadsheets/d/18Y4QgS2P5FxRPShKPsapPfBptUBs_q17LZ_i7tePeLo/edit#gid=1821727344 


Обзор методов есть здесь https://docs.google.com/document/d/1PKl-MIZ2pICjrYl76PR2xnB9WcpqUpbl4WafL8-nZT8/edit 

В принципе методы у всех схожие, есть энкодер, состоящий из эмбеддинга + вариативных частей в виде доп признаков и последующих слоёв, которые могут быть LSTM/Conv/Transformer или что-то подобное. Так кодируется входное упоминание или текст. Потом есть эмбеддинг для словаря, который может быть обучаемый, или предобученный (разными способами), пошареный с входным эмбеддингом. И есть процедура выбора подходящего термина из словаря, по метрике или слоем каким-то (аттеншен или просто денс, то есть скалярным произведением).

Задачи.

2.1 Добавить в таблицу с корпусами информацию по TwiMed, PsyTar, TwADR-S, TwADR-L, n2c2 2019, SemEval-2015. 

2.2 Было бы неплохо собрать в таблицу или схему как раз все эти вариации, которые возможно по следующим категориям:
- как обучается входной эмбеддинг
- что идёт после эмбеддинга
- какие есть дополнительные входные признаки кроме текста, пропускаемого через эмбеддинг
- как обучается эмбеддинг словаря
- какая процедура линкования двух векторов.

2.3 Можно еще вот здесь что-то подобрать, посмотреть: https://paperswithcode.com/task/entity-linking , потому что суть задачи вроде та же. Но там нет медицинских корпусов и по каждому корпусу по одной работе. Так что как выбирать и сравнивать не понятно, если только всё читать и на свой вкус выбирать.

2.4 Подготовить другие корпуса
Из тех, корпусов, которые собраны в таблице https://docs.google.com/spreadsheets/d/18Y4QgS2P5FxRPShKPsapPfBptUBs_q17LZ_i7tePeLo/edit#gid=1821727344 выбрать какие-то на которых будем сравниваться и привести их к нашему формату.

# Разработка собственной системы.
## Формализация задачи
Ну основная формализация, которую можно рассматривать такая: есть текст, есть в нём упоминания выделенные, надо в соответствие каждому упоминанию выбрать код который проставили разметчики.

Какие могут быть вариации:
 - можно рассматривать только упоминание без окружающего текста. Так задача будет сложней, но возможно это единственный вариант, который можно рассматривать для корпуса росздрава (а может и нет, там есть какая-то доп инфа, если адр в ней упомянут, то можно и этот текст задействовать).
- можно наборот классифицировать (мультилейбл) весь текст, указывая, какие коды в нём есть, без их точного положения в тексте. Это может тоже в росздраве с доп. инфой сработает. 
- Отдельно по каждому типу, или по каждому словарю, или всё вместе сразу?
## Метрика
Самый простой и правильный вариант: F1 score по классам. Micro и Macro. Precision и recall наверно не нужны. 

Варианты для исследования:
- При второй формулировке задачи можно еще рассматривать только уникальные упоминания, но это лишняя заморочка. Не стоит на это тратить время.
- В некоторых работах еще делают оценку с учётом ограниченного количество классов для выбора,например только те, которые есть в корпусе. 
- Мы еще делали оценки по тому, попал ли  нужный класс в топ N выдаваемых классов.
- Можно еще с учётом иерархической структуры как-то считать, например точности по определённому уровню классификации, типо по PT такая ошибка, по HGLT ниже.
- Таблицы перепутываний классов имеет смысл строить, чтоб определять какие между собой путаются. 

Задачи.
1.1 определиться, будем ли мы рассматривать разные формализации, или только одну.

1.2  Выбрать какие метрики мы планируем использовать. Все или только самые важные.

1.3. Может глянуть, где другие метрики используют?

1.3 написать скрипт метрики. На вход тест, трейн или валид корпус в эталонном формате + файл предиктов (в каком формате?), на выходе нужные метрики.
## Разработка метода.
Для создания модели под наш корпус имеет смысл проводить эксперименты по следующим направлениям
- как обучается входной эмбеддинг. Эмбеддинг языковой модели обучается сразу на нашем корпусе, или делать предобучение на целевую тематику.
- что идёт после эмбеддинга. ну тут экспериментировать особо нечего, трансформеры, вопрос только в размере. Наверно tiny bert, чтоб быстрее и проще.
- какие есть дополнительные входные признаки кроме текста, пропускаемого через эмбеддинг
- как обучается эмбеддинг словаря. Самый простой вариант, можно как бейзлайн - веса шарятся с входным эмбеддингом. Как вариант - из гугла напарсить определений к терминам или какой-то корпус с терминами и соответственно из них языковой моделью получить эмбеддинги.







