{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dynamic-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models, InputExample, losses, evaluation\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from hyperopt import fmin, tpe, space_eval, STATUS_OK\n",
    "from hyperopt import hp\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mighty-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./best_srubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complicated-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./best_srubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recovered-license",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cfdfa8803b49fe8b78ba02aa302ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d558cb8a874c449f413e6b8323a0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd972bb809d04091a055c2c6791a8308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859306b6dbdc47e7ab6ac3e7acd4ec24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329347d75df04045882e4f965d9e28bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d155b908dd741c4819a0756027866e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd08539ce264d2ebcd418f165d362ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00040ca8a156434b98f215d219a58fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mCosineSimilarityLoss(net)\n\u001b[1;32m     36\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m num_epochs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mevaluator_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwarmupcosinewithhardrestarts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#evaluator = evaluator_val, \\\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./best_srubert/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m#evaluation_steps=len(train_dataloader))\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPearson correlation on a dev set in the end of the training:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/latest_torch/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:713\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_model(features, labels)\n\u001b[0;32m--> 713\u001b[0m     \u001b[43mloss_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(loss_model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n\u001b[1;32m    715\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/latest_torch/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/latest_torch/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 8\n",
    "lr = 1.85e-6\n",
    "\n",
    "word_embedding_model = models.Transformer('DeepPavlov/rubert-base-cased', max_seq_length=512)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "#в обычных языковых моделях добавляется pooler layer с Tanh activation, здесь тоже его добавлю\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=pooling_model.get_sentence_embedding_dimension(), activation_function=nn.Tanh())\n",
    "\n",
    "net = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "sts_df_train = pd.read_csv('./STS_train.csv')\n",
    "sts_df_test = pd.read_csv('./STS_test.csv')\n",
    "sts_df_val = pd.read_csv('./STS_dev.csv')\n",
    "\n",
    "sts_ds_train = []\n",
    "#sts_ds_test = []\n",
    "sts_ds_val = []\n",
    "\n",
    "for row in sts_df_train.iterrows():\n",
    "    sts_ds_train.append(InputExample(texts=[row[1]['sentence1'], row[1]['sentence2']], label=row[1]['similarity_score']))\n",
    "\n",
    "evaluator_val = evaluation.EmbeddingSimilarityEvaluator(sts_df_val['sentence1'], \n",
    "                                                        sts_df_val['sentence2'], \n",
    "                                                        sts_df_val['similarity_score'])\n",
    "evaluator_test = evaluation.EmbeddingSimilarityEvaluator(sts_df_test['sentence1'], \n",
    "                                                        sts_df_test['sentence2'], \n",
    "                                                        sts_df_test['similarity_score'])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(sts_ds_train, shuffle=False, batch_size=batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(net)\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n",
    "\n",
    "net.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "        steps_per_epoch = len(train_dataloader), \\\n",
    "        optimizer_params = {'lr': lr}, \\\n",
    "        warmup_steps=warmup_steps, \\\n",
    "        evaluator = evaluator_val, \\\n",
    "        scheduler = 'warmupcosinewithhardrestarts', \\\n",
    "        #evaluator = evaluator_val, \\\n",
    "        save_best_model=True, \\\n",
    "        epochs=num_epochs, \\\n",
    "        output_path = './best_srubert/')\n",
    "        #evaluation_steps=len(train_dataloader))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-grounds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation on a dev set in the end of the training:\n",
      "0.5487853411608976\n",
      "Pearson correlation on a test set in the end of the training:\n",
      "0.44158279612124124\n",
      "\n",
      "\n",
      "\n",
      "Pearson correlation on a dev set of the best model:\n"
     ]
    }
   ],
   "source": [
    "print('Pearson correlation on a dev set in the end of the training:')\n",
    "print(net.evaluate(evaluator_val))\n",
    "print('Pearson correlation on a test set in the end of the training:')\n",
    "print(net.evaluate(evaluator_test))\n",
    "print('\\n\\n')\n",
    "net = SentenceTransformer('./best_srubert')\n",
    "print('Pearson correlation on a dev set of the best model:')\n",
    "print(net.evaluate(evaluator_val))\n",
    "print('Pearson correlation on a test set of the best model:')\n",
    "print(net.evaluate(evaluator_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-detail",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normalization env",
   "language": "python",
   "name": "latest_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
