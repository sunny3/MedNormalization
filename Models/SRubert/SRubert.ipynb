{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brazilian-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models, InputExample, losses, evaluation\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from hyperopt import fmin, tpe, space_eval, STATUS_OK\n",
    "from hyperopt import hp\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "norwegian-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./best_srubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wanted-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./best_srubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 8\n",
    "lr = 1.85e-6\n",
    "\n",
    "word_embedding_model = models.Transformer('DeepPavlov/rubert-base-cased', max_seq_length=512)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "#в обычных языковых моделях добавляется pooler layer с Tanh activation, здесь тоже его добавлю\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=pooling_model.get_sentence_embedding_dimension(), activation_function=nn.Tanh())\n",
    "\n",
    "net = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "sts_df_train = pd.read_csv('./STS_train.csv')\n",
    "sts_df_test = pd.read_csv('./STS_test.csv')\n",
    "sts_df_val = pd.read_csv('./STS_dev.csv')\n",
    "\n",
    "sts_ds_train = []\n",
    "#sts_ds_test = []\n",
    "sts_ds_val = []\n",
    "\n",
    "for row in sts_df_train.iterrows():\n",
    "    sts_ds_train.append(InputExample(texts=[row[1]['sentence1'], row[1]['sentence2']], label=row[1]['similarity_score']))\n",
    "\n",
    "evaluator_val = evaluation.EmbeddingSimilarityEvaluator(sts_df_val['sentence1'], \n",
    "                                                        sts_df_val['sentence2'], \n",
    "                                                        sts_df_val['similarity_score'])\n",
    "evaluator_test = evaluation.EmbeddingSimilarityEvaluator(sts_df_test['sentence1'], \n",
    "                                                        sts_df_test['sentence2'], \n",
    "                                                        sts_df_test['similarity_score'])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(sts_ds_train, shuffle=False, batch_size=batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(net)\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n",
    "\n",
    "net.fit(train_objectives=[(train_dataloader, train_loss)], \n",
    "        steps_per_epoch = len(train_dataloader), \\\n",
    "        optimizer_params = {'lr': lr}, \\\n",
    "        warmup_steps=warmup_steps, \\\n",
    "        evaluator = evaluator_val, \\\n",
    "        scheduler = 'warmupcosinewithhardrestarts', \\\n",
    "        #evaluator = evaluator_val, \\\n",
    "        save_best_model=True, \\\n",
    "        epochs=num_epochs, \\\n",
    "        output_path = './best_srubert/')\n",
    "        #evaluation_steps=len(train_dataloader))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afraid-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation on a dev set in the end of the training:\n",
      "0.5487853411608976\n",
      "Pearson correlation on a test set in the end of the training:\n",
      "0.44158279612124124\n",
      "\n",
      "\n",
      "\n",
      "Pearson correlation on a dev set of the best model:\n",
      "0.6888704258981811\n",
      "Pearson correlation on a test set of the best model:\n",
      "0.59109003179712\n"
     ]
    }
   ],
   "source": [
    "print('Pearson correlation on a dev set in the end of the training:')\n",
    "print(net.evaluate(evaluator_val))\n",
    "print('Pearson correlation on a test set in the end of the training:')\n",
    "print(net.evaluate(evaluator_test))\n",
    "print('\\n\\n')\n",
    "net = SentenceTransformer('./best_srubert')\n",
    "print('Pearson correlation on a dev set of the best model:')\n",
    "print(net.evaluate(evaluator_val))\n",
    "print('Pearson correlation on a test set of the best model:')\n",
    "print(net.evaluate(evaluator_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-detail",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normalization env",
   "language": "python",
   "name": "latest_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
