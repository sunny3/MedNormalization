{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mental-testing",
   "metadata": {},
   "source": [
    "<h3>Векторизуем словарь Meddra</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exterior-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorization import ConceptVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial-female",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "CV = ConceptVectorizer('cointegrated/rubert-tiny2', '../../Data/External/pt_rus.asc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unexpected-assets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting concept embeddings in mean_pooling mode...\n"
     ]
    }
   ],
   "source": [
    "CV.fit_transform(mode='mean_pooling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optical-amazon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5571,  0.4180,  0.3808,  ...,  0.5321,  0.8140, -1.6517],\n",
       "        [-0.0364, -0.3752,  0.2873,  ...,  0.4556,  0.6893, -0.5252],\n",
       "        [-0.1648, -0.4934,  0.5567,  ...,  0.6926,  0.2847, -0.4295],\n",
       "        ...,\n",
       "        [-0.0708,  0.0196,  0.7664,  ..., -0.2933,  0.9758, -1.3564],\n",
       "        [-0.2319, -0.6193, -0.0307,  ...,  0.4852,  0.1158, -0.8784],\n",
       "        [ 0.0246, -0.9299,  0.0626,  ...,  0.2106,  0.2250, -0.9511]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.thesaurus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ordered-gates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23954\n"
     ]
    }
   ],
   "source": [
    "print(len(CV.thesaurus_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deluxe-palmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean_pooling'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.vectorization_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-nigeria",
   "metadata": {},
   "source": [
    "<h3>Создадим датасет RDR</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pacific-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cheap-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "with jsonlines.open('../../Data/Raw/medNorm_14012022.jsonlines') as reader:\n",
    "    for obj in reader:\n",
    "        ds.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pointed-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(ds, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hydraulic-sydney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего фраз в трейне: 3476\n",
      "Всего фраз в тесте: 1750\n",
      "Уникальных фраз в трейне: 1204\n",
      "Уникальных фраз в тесте: 750\n",
      "162 концептов не входящих либо в трейн, либо в тест\n",
      "53 концептов, которые есть в тесте, но нет в трейне\n",
      "109 концептов, которые есть в трейне, но нет в тесте\n"
     ]
    }
   ],
   "source": [
    "#выцепим фразы с нормализацией по Meddra без их контекста\n",
    "\n",
    "train_phrases = []\n",
    "train_concepts = []\n",
    "\n",
    "test_phrases = []\n",
    "test_concepts = []\n",
    "\n",
    "for review in X_train:\n",
    "    for ent in review['objects']['MedEntity']:\n",
    "        if 'MedDRA' in ent.keys() and ent['MedDRA']!='':\n",
    "            #try:\n",
    "            train_concepts.append(CV.meddra_term_to_meddra_code[ent['MedDRA'].split('|')[0]])\n",
    "            #except:\n",
    "            #markup_errors+=1\n",
    "            #continue\n",
    "            train_phrases.append(ent['text'])\n",
    "            \n",
    "            \n",
    "for review in X_test:\n",
    "    for ent in review['objects']['MedEntity']:\n",
    "        if 'MedDRA' in ent.keys() and ent['MedDRA']!='':\n",
    "            #try:\n",
    "            test_concepts.append(CV.meddra_term_to_meddra_code[ent['MedDRA'].split('|')[0]])\n",
    "            #except:\n",
    "            #    markup_errors+=1\n",
    "            #    continue\n",
    "            test_phrases.append(ent['text'])\n",
    "            \n",
    "print('Всего фраз в трейне: %s'%len(train_phrases))\n",
    "print('Всего фраз в тесте: %s'%len(test_phrases))\n",
    "\n",
    "print('Уникальных фраз в трейне: %s'%len(set(train_phrases)))\n",
    "print('Уникальных фраз в тесте: %s'%len(set(test_phrases)))\n",
    "\n",
    "#Посмотрим на статистику разбиения\n",
    "print('%s концептов не входящих либо в трейн, либо в тест'%len(set.union(set(train_concepts), set(test_concepts)) - set.intersection(set(test_concepts), set(train_concepts))))\n",
    "print('%s концептов, которые есть в тесте, но нет в трейне'%len(set(test_concepts) - set(train_concepts)))\n",
    "print('%s концептов, которые есть в трейне, но нет в тесте'%len(set(train_concepts) - set(test_concepts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "regular-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MedNormDataset\n",
    "\n",
    "RDR_train = MedNormDataset(train_phrases, train_concepts, CV, use_cuda=True)\n",
    "RDR_test = MedNormDataset(test_phrases, test_concepts, CV, use_cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-fellow",
   "metadata": {},
   "source": [
    "<h2>Сама модель</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "studied-subsection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing Net: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing Net from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Net from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net loaded\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "#Динамический импорт класса нужной PreTrained модели по автоконфигурации\n",
    "from transformers import AutoConfig\n",
    "\n",
    "#Все, что нужно указать\n",
    "model_path = 'cointegrated/rubert-tiny2'\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(model_path)\n",
    "ConfigModelClass = cfg.__class__\n",
    "PreTrainedModelClassName = ConfigModelClass.__name__.replace('Config', 'PreTrainedModel')\n",
    "ModelClassName = cfg.__class__.__name__.replace('Config', 'Model')\n",
    "exec(\"from transformers import %s as PreTrainedModelClass\"%PreTrainedModelClassName)\n",
    "exec(\"from transformers import %s as ModelClass\"%ModelClassName)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-8)\n",
    "\n",
    "normalized_embs = CV.thesaurus_embeddings.norm(dim=1)[:, None]\n",
    "normalized_embs = CV.thesaurus_embeddings / torch.clamp(normalized_embs, min=1e-8)\n",
    "normalized_embs = normalized_embs.transpose(0, 1)\n",
    "normalized_embs = normalized_embs.to(device)\n",
    "\n",
    "class Net(PreTrainedModelClass):\n",
    "    def __init__(self, config: ConfigModelClass):\n",
    "        super(Net, self).__init__(config)\n",
    "        self.bert = ModelClass(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.bert(**x)\n",
    "        #with torch.no_grad():\n",
    "        x = mean_pooling(emb, x['attention_mask'])\n",
    "        #имеем две матрицы x - (batch_size, emb_size) и thesaurus_embeddings - (thesaurus_size, emb_size)\n",
    "        #надо посчитать косинусную близость близость между каждым вектором x и каждым вложением из тезауруса\n",
    "        #решение: https://stackoverflow.com/questions/50411191/how-to-compute-the-cosine-similarity-in-pytorch-for-all-rows-in-a-matrix-with-re\n",
    "        x_n = x.norm(dim=1)[:, None] \n",
    "        x_n = x / torch.clamp(x_n, min=1e-8)\n",
    "        #b_norm = CV.thesaurus_embeddings / torch.clamp(b_, min=1e-9)\n",
    "        cos_sim = torch.mm(x_n, normalized_embs)\n",
    "        x = F.softmax(cos_sim, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net.from_pretrained('cointegrated/rubert-tiny2', config=cfg)\n",
    "net.to(device)\n",
    "print('Net loaded')\n",
    "\n",
    "#net2 = Net.from_pretrained('cointegrated/rubert-tiny2', config=cfg)\n",
    "#net2.to(device)\n",
    "#print('Net loaded')\n",
    "#normalized_embs.to(device)\n",
    "\n",
    "#for param in net.parameters():\n",
    "#    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-functionality",
   "metadata": {},
   "source": [
    "<h2>Обучение модели</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "desirable-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters())\n",
    "#optimizer2 = optim.AdamW(net2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "running-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-survival",
   "metadata": {},
   "source": [
    "Для большей детерменированности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "agricultural-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n",
    "torch.use_deterministic_algorithms(mode=False)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "decimal-sister",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1738/1738 [00:39<00:00, 44.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "batch_size=2\n",
    "trainloader = torch.utils.data.DataLoader(RDR_train, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "net.train()\n",
    "for epoch in range(1, 2):\n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = data['tokenized_phrases']\n",
    "            labels = data['one_hot_labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if device=='cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            #чтобы посчитать accuracy нужно конвертнуть в one-hot\n",
    "            \n",
    "            #max_idx = torch.argmax(outputs, 1, keepdim=True)\n",
    "            #one_hot = torch.FloatTensor(outputs.shape).to(device)\n",
    "            #one_hot.zero_()\n",
    "            #one_hot.scatter_(1, max_idx, 1)\n",
    "            #correct = torch.all(torch.eq(labels, one_hot),  dim=1).sum().item()\n",
    "\n",
    "            #correct = (one_hot == labels)\n",
    "            #accuracy = correct / batch_size\n",
    "            #tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-investigation",
   "metadata": {},
   "source": [
    "<h2>Тест модели с mean_pooling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "starting-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [00:06<00:00, 252.77batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5891428571428572"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "net.eval()\n",
    "\n",
    "model_answers=[]\n",
    "real_answers=[]\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(RDR_test, batch_size=1,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "with tqdm(testloader, unit=\"batch\") as eval_process:\n",
    "    for data in eval_process:\n",
    "\n",
    "        #tepoch.set_description(f\"Progress\")\n",
    "\n",
    "        inputs = data['tokenized_phrases']\n",
    "        labels = data['one_hot_labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_meddra_code = CV.meddra_codes[net(inputs).argmax()]\n",
    "\n",
    "\n",
    "        model_answers.append(pred_meddra_code)\n",
    "        real_answers.append(data['label_codes'])\n",
    "\n",
    "f1_score(real_answers, model_answers, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5891428571428572, 0.5891428571428572, 0.5891428571428572, 0.5891428571428572"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-gibson",
   "metadata": {},
   "source": [
    "<h2> Тест необученной модели (на всякий случай) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "neural-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [00:05<00:00, 294.15batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17142857142857143"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "#net.train()\n",
    "net.eval()\n",
    "\n",
    "model_answers=[]\n",
    "real_answers=[]\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(RDR_test, batch_size=1,\n",
    "                                          shuffle=False)\n",
    "\n",
    "with tqdm(testloader, unit=\"batch\") as eval_process:\n",
    "    for data in eval_process:\n",
    "\n",
    "        #tepoch.set_description(f\"Progress\")\n",
    "\n",
    "        inputs = data['tokenized_phrases']\n",
    "        labels = data['one_hot_labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_meddra_code = CV.meddra_codes[net(inputs).argmax()]\n",
    "\n",
    "\n",
    "        model_answers.append(pred_meddra_code)\n",
    "        real_answers.append(data['label_codes'])\n",
    "\n",
    "f1_score(real_answers, model_answers, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-productivity",
   "metadata": {},
   "source": [
    "<h2>Инференс</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "greatest-memorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase: температурой 37,8\n",
      "model: Пирексия\n",
      "real: Пирексия\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "i = randint(1, len(RDR_test))\n",
    "\n",
    "net.eval()\n",
    "\n",
    "phrase = {k: tensor.unsqueeze(0) for k, tensor in RDR_test[i]['tokenized_phrases'].items()}\n",
    "concept = RDR_test[i]['label_codes']\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_answer = CV.meddra_codes[net(phrase).argmax()]\n",
    "    \n",
    "\n",
    "\n",
    "print('phrase: %s'%RDR_test[i]['phrases'])\n",
    "print('model: %s'%CV.meddra_code_to_meddra_term[model_answer])\n",
    "\n",
    "\n",
    "\n",
    "print('real: %s'%RDR_test[i]['label_terms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-brisbane",
   "metadata": {},
   "source": [
    "<h2>Сохранение и загрузка модели</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "joined-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './cadec_SoTa_on_RDR_rubert_right_exp.pt')\n",
    "torch.save(optimizer.state_dict(), './cadec_SoTa_on_RDR_rubert_right_exp_opt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mineral-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = torch.load('./cadec_SoTa_on_RDR_rubert_right_exp.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-tulsa",
   "metadata": {},
   "source": [
    "Покажем, что это ТА ЖЕ модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "recovered-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [00:06<00:00, 259.93batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5891428571428572"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.eval()\n",
    "\n",
    "model_answers=[]\n",
    "real_answers=[]\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(RDR_test, batch_size=1,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "with tqdm(testloader, unit=\"batch\") as eval_process:\n",
    "    for data in eval_process:\n",
    "\n",
    "\n",
    "        inputs = data['tokenized_phrases']\n",
    "        labels = data['one_hot_labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_meddra_code = CV.meddra_codes[the_model(inputs).argmax()]\n",
    "\n",
    "\n",
    "        model_answers.append(pred_meddra_code)\n",
    "        real_answers.append(data['label_codes'])\n",
    "\n",
    "f1_score(real_answers, model_answers, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-bridges",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest_torch",
   "language": "python",
   "name": "latest_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
