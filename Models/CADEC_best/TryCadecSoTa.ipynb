{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approved-statement",
   "metadata": {},
   "source": [
    "<h3>Векторизуем словарь Meddra</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorization import ConceptVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retired-local",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "CV = ConceptVectorizer('cointegrated/rubert-tiny2', '../../Data/External/pt_rus.asc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dress-martin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting concept embeddings...\n"
     ]
    }
   ],
   "source": [
    "CV.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alive-benefit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5571,  0.4180,  0.3808,  ...,  0.5321,  0.8140, -1.6517],\n",
       "        [-0.0364, -0.3752,  0.2873,  ...,  0.4556,  0.6893, -0.5252],\n",
       "        [-0.1648, -0.4934,  0.5567,  ...,  0.6926,  0.2847, -0.4295],\n",
       "        ...,\n",
       "        [-0.0708,  0.0196,  0.7664,  ..., -0.2933,  0.9758, -1.3564],\n",
       "        [-0.2319, -0.6193, -0.0307,  ...,  0.4852,  0.1158, -0.8784],\n",
       "        [ 0.0246, -0.9299,  0.0626,  ...,  0.2106,  0.2250, -0.9511]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.thesaurus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blocked-stationery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23954\n"
     ]
    }
   ],
   "source": [
    "print(len(CV.meddra_codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-drive",
   "metadata": {},
   "source": [
    "<h3>Создадим датасет RDR</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "copyrighted-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agreed-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "with jsonlines.open('../../Data/Raw/medNorm_14012022.jsonlines') as reader:\n",
    "    for obj in reader:\n",
    "        ds.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arabic-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(ds, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "assisted-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего фраз в трейне: 3476\n",
      "Всего фраз в тесте: 1750\n",
      "Уникальных фраз в трейне: 1204\n",
      "Уникальных фраз в тесте: 750\n",
      "162 концептов не входящих либо в трейн, либо в тест\n",
      "53 концептов, которые есть в тесте, но нет в трейне\n",
      "109 концептов, которые есть в трейне, но нет в тесте\n"
     ]
    }
   ],
   "source": [
    "#выцепим фразы с нормализацией по Meddra без их контекста\n",
    "\n",
    "train_phrases = []\n",
    "train_concepts = []\n",
    "\n",
    "test_phrases = []\n",
    "test_concepts = []\n",
    "\n",
    "#markup_errors = 0\n",
    "\n",
    "for review in X_train:\n",
    "    for ent in review['objects']['MedEntity']:\n",
    "        if 'MedDRA' in ent.keys() and ent['MedDRA']!='':\n",
    "            #try:\n",
    "            train_concepts.append(CV.meddra_term_to_meddra_code[ent['MedDRA'].split('|')[0]])\n",
    "            #except:\n",
    "            #markup_errors+=1\n",
    "            #continue\n",
    "            train_phrases.append(ent['text'])\n",
    "            \n",
    "            \n",
    "for review in X_test:\n",
    "    for ent in review['objects']['MedEntity']:\n",
    "        if 'MedDRA' in ent.keys() and ent['MedDRA']!='':\n",
    "            #try:\n",
    "            test_concepts.append(CV.meddra_term_to_meddra_code[ent['MedDRA'].split('|')[0]])\n",
    "            #except:\n",
    "            #    markup_errors+=1\n",
    "            #    continue\n",
    "            test_phrases.append(ent['text'])\n",
    "            \n",
    "#print('Число не найденных в словаре концептов: %s'%markup_errors)\n",
    "print('Всего фраз в трейне: %s'%len(train_phrases))\n",
    "print('Всего фраз в тесте: %s'%len(test_phrases))\n",
    "\n",
    "print('Уникальных фраз в трейне: %s'%len(set(train_phrases)))\n",
    "print('Уникальных фраз в тесте: %s'%len(set(test_phrases)))\n",
    "\n",
    "#Посмотрим на статистику разбиения\n",
    "print('%s концептов не входящих либо в трейн, либо в тест'%len(set.union(set(train_concepts), set(test_concepts)) - set.intersection(set(test_concepts), set(train_concepts))))\n",
    "print('%s концептов, которые есть в тесте, но нет в трейне'%len(set(test_concepts) - set(train_concepts)))\n",
    "print('%s концептов, которые есть в трейне, но нет в тесте'%len(set(train_concepts) - set(test_concepts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "textile-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#векторизуем для обучения и теста\n",
    "from copy import copy\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "train_y = []\n",
    "test_y = []\n",
    "\n",
    "empty_vec = [0]*len(CV.meddra_codes)\n",
    "for concept in train_concepts:\n",
    "    y = copy(empty_vec)\n",
    "    y[CV.meddra_codes.index(concept)] = 1\n",
    "    train_y.append(y)\n",
    "    \n",
    "for concept in test_concepts:\n",
    "    y = copy(empty_vec)\n",
    "    y[CV.meddra_codes.index(concept)] = 1\n",
    "    test_y.append(y)\n",
    "\n",
    "train_y = torch.tensor(train_y)\n",
    "test_y = torch.tensor(test_y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "checked-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n",
    "train_X = tokenizer(train_phrases, padding=True, truncation=True, return_tensors='pt')\n",
    "test_X = tokenizer(test_phrases, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-reynolds",
   "metadata": {},
   "source": [
    "Проверка векторизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "talented-terror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "отек тканей\n",
      "Отек кожи\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "i = randint(1, train_y.size()[0])\n",
    "#phrase = train_X[i]\n",
    "concept = train_y[i]\n",
    "\n",
    "meddra_code = CV.meddra_codes[concept.argmax()]\n",
    "\n",
    "print(train_phrases[i])\n",
    "print(CV.meddra_code_to_meddra_term[meddra_code])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-condition",
   "metadata": {},
   "source": [
    "Демонстрация того, как отрабатывает косинусная близость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "closed-mechanics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.7345e-05, 4.3767e-05, 4.5936e-05,  ..., 5.2762e-05, 4.4660e-05,\n",
       "         4.3657e-05],\n",
       "        [6.7345e-05, 4.3767e-05, 4.5936e-05,  ..., 5.2762e-05, 4.4660e-05,\n",
       "         4.3657e-05],\n",
       "        [6.7345e-05, 4.3767e-05, 4.5936e-05,  ..., 5.2762e-05, 4.4660e-05,\n",
       "         4.3657e-05],\n",
       "        ...,\n",
       "        [6.7345e-05, 4.3767e-05, 4.5936e-05,  ..., 5.2762e-05, 4.4660e-05,\n",
       "         4.3657e-05],\n",
       "        [6.7345e-05, 4.3767e-05, 4.5936e-05,  ..., 5.2762e-05, 4.4660e-05,\n",
       "         4.3657e-05],\n",
       "        [6.7345e-05, 4.3767e-05, 4.5936e-05,  ..., 5.2762e-05, 4.4660e-05,\n",
       "         4.3657e-05]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "a = CV.thesaurus_embeddings[0].unsqueeze(0).repeat(32, 1)\n",
    "b = CV.thesaurus_embeddings#.unsqueeze(0).repeat(32, 1, 1)\n",
    "\n",
    "a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "a_norm = a / torch.clamp(a_n, min=1e-6)\n",
    "b_norm = b / torch.clamp(b_n, min=1e-6)\n",
    "sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "F.softmax(sim_mt, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-range",
   "metadata": {},
   "source": [
    "<h2>Сама модель</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "involved-copyright",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "model = AutoModel.from_pretrained('cointegrated/rubert-tiny2')\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-9)\n",
    "cfg = AutoConfig.from_pretrained('cointegrated/rubert-tiny2')\n",
    "\n",
    "#encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "#with torch.no_grad():\n",
    "#    model_output = model(**encoded_input)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "normalized_embs = CV.thesaurus_embeddings.norm(dim=1)[:, None]\n",
    "normalized_embs = CV.thesaurus_embeddings / torch.clamp(normalized_embs, min=1e-9)\n",
    "normalized_embs = normalized_embs.transpose(0, 1)\n",
    "\n",
    "class Net(BertPreTrainedModel):\n",
    "    def __init__(self):\n",
    "        #super().__init__()\n",
    "        super(Net, self).__init__(cfg)\n",
    "        self.emb = BertModel(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(**x)\n",
    "        #with torch.no_grad():\n",
    "        x = mean_pooling(emb, x['attention_mask'])\n",
    "        #имеем две матрицы x - (batch_size, emb_size) и thesaurus_embeddings - (thesaurus_size, emb_size)\n",
    "        #надо посчитать косинусную близость близость между каждым вектором x и каждым вложением из тезауруса\n",
    "        #решение: https://stackoverflow.com/questions/50411191/how-to-compute-the-cosine-similarity-in-pytorch-for-all-rows-in-a-matrix-with-re\n",
    "        x_n = x.norm(dim=1)[:, None] \n",
    "        x_n = x / torch.clamp(x_n, min=1e-9)\n",
    "        #b_norm = CV.thesaurus_embeddings / torch.clamp(b_, min=1e-9)\n",
    "        cos_sim = torch.mm(x_n, normalized_embs)\n",
    "        x = F.softmax(cos_sim, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-pakistan",
   "metadata": {},
   "source": [
    "<h2>Обучение модели</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affected-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interim-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedNormDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        if y.dtype==torch.int64:\n",
    "            self.y = y.to(torch.float32)\n",
    "        else:\n",
    "            self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X['input_ids'].size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #if torch.is_tensor(idx):\n",
    "        #    idx = idx.tolist()\n",
    "        sample = {'tokenized_phrases': {'input_ids': self.X['input_ids'][idx], \n",
    "                                        'token_type_ids': self.X['token_type_ids'][idx],\n",
    "                                        'attention_mask': self.X['attention_mask'][idx]},\n",
    "                  'label': self.y[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "connected-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1738/1738 [03:41<00:00,  7.85batch/s, accuracy=50, loss=10.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "batch_size=2\n",
    "RDR_ds = MedNormDataset(X=train_X, y=train_y)\n",
    "trainloader = torch.utils.data.DataLoader(RDR_ds, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "net.train()\n",
    "for epoch in range(1, 2):\n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = data['tokenized_phrases']\n",
    "            labels = data['label']\n",
    "\n",
    "            optimizer.zero_grad()    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            #чтобы посчитать accuracy нужно конвертнуть в one-hot\n",
    "            max_idx = torch.argmax(outputs, 1, keepdim=True)\n",
    "            one_hot = torch.FloatTensor(outputs.shape)\n",
    "            one_hot.zero_()\n",
    "            one_hot.scatter_(1, max_idx, 1)\n",
    "            correct = torch.all(torch.eq(labels, one_hot),  dim=1).sum().item()\n",
    "\n",
    "            #correct = (one_hot == labels)\n",
    "            accuracy = correct / batch_size\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            \n",
    "             #optimizer.zero_grad()\n",
    "             #output = model(data)\n",
    "             #predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "             #loss = F.nll_loss(output, target)\n",
    "             #correct = (predictions == target).sum().item()\n",
    "             #accuracy = correct / batch_size\n",
    "\n",
    "             #loss.backward()\n",
    "             #optimizer.step()\n",
    "\n",
    "             #tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "             #sleep(0.1)\n",
    "\n",
    "#phbar = trange(trainloader, desc='Batch loss 0')\n",
    "#f = open('log.txt', 'w')\n",
    "#for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    #running_loss = 0.0\n",
    "    #for data in phbar:#enumerate((trainloader, 0), desc='Batch loss 0' ) \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        #inputs = data['tokenized_phrases']\n",
    "        #labels = data['label']\n",
    "        #print(inputs['input_ids'][0])\n",
    "        # zero the parameter gradients\n",
    "        #optimizer.zero_grad()    \n",
    "        # forward + backward + optimize\n",
    "        #outputs = net(inputs)\n",
    "        #loss = criterion(outputs, labels)\n",
    "        #f.write('%s\\n'%loss.item())\n",
    "        #phbar.set_description('Batch loss %s'%loss.item())\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        # print statistics\n",
    "        #running_loss += loss.item()\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        #    running_loss = 0.0\n",
    "#f.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-municipality",
   "metadata": {},
   "source": [
    "<h2>Тест</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greek-worth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "net.eval()\n",
    "\n",
    "model_answers=[]\n",
    "real_answers=[]\n",
    "\n",
    "for i, phrase in enumerate(test_phrases):\n",
    "    #i = randint(1, train_y.size()[0])\n",
    "    encoded_phrase = {'input_ids': test_X['input_ids'][i].unsqueeze(0), \n",
    "              'token_type_ids': test_X['token_type_ids'][i].unsqueeze(0),\n",
    "              'attention_mask': test_X['attention_mask'][i].unsqueeze(0)}\n",
    "\n",
    "    concept = test_y[i]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        meddra_code = CV.meddra_codes[net(encoded_phrase).argmax()]\n",
    "\n",
    "\n",
    "    model_answers.append(meddra_code)\n",
    "    #print('phrase: %s'%phrase)\n",
    "    #print('model: %s'%CV.meddra_code_to_meddra_term[meddra_code])\n",
    "\n",
    "\n",
    "\n",
    "    meddra_code = CV.meddra_codes[concept.argmax()]\n",
    "    real_answers.append(meddra_code)\n",
    "    #print(train_phrases[i])\n",
    "    #print('real: %s'%CV.meddra_code_to_meddra_term[meddra_code])\n",
    "    \n",
    "#print(classification_report(real_answers, model_answers, zero_division=0))\n",
    "f1_score(real_answers, model_answers, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-guyana",
   "metadata": {},
   "source": [
    "<h2>Инференс</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "finished-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase: температура 40\n",
      "model: Повышенная температура тела\n",
      "real: Пирексия\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "i = randint(1, train_y.size()[0])\n",
    "\n",
    "net.eval()\n",
    "phrase = {'input_ids': train_X['input_ids'][i].unsqueeze(0), \n",
    "          'token_type_ids': train_X['token_type_ids'][i].unsqueeze(0),\n",
    "          'attention_mask': train_X['attention_mask'][i].unsqueeze(0)}\n",
    "\n",
    "concept = train_y[i]\n",
    "\n",
    "with torch.no_grad():\n",
    "    meddra_code = CV.meddra_codes[net(phrase).argmax()]\n",
    "    \n",
    "\n",
    "\n",
    "print('phrase: %s'%train_phrases[i])\n",
    "print('model: %s'%CV.meddra_code_to_meddra_term[meddra_code])\n",
    "\n",
    "\n",
    "\n",
    "meddra_code = CV.meddra_codes[concept.argmax()]\n",
    "\n",
    "#print(train_phrases[i])\n",
    "print('real: %s'%CV.meddra_code_to_meddra_term[meddra_code])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-maldives",
   "metadata": {},
   "source": [
    "<h2>Сохранение и загрузка модели</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sound-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./cadec_SoTa_on_RDR_rubert_tiny/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "graduate-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './cadec_SoTa_on_RDR_rubert_tiny.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "specialized-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = torch.load('./cadec_SoTa_on_RDR_rubert_tiny.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-startup",
   "metadata": {},
   "source": [
    "Покажем, что это ТА ЖЕ модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "backed-equation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "the_model.eval()\n",
    "\n",
    "model_answers=[]\n",
    "real_answers=[]\n",
    "\n",
    "for i, phrase in enumerate(test_phrases):\n",
    "    #i = randint(1, train_y.size()[0])\n",
    "    encoded_phrase = {'input_ids': test_X['input_ids'][i].unsqueeze(0), \n",
    "              'token_type_ids': test_X['token_type_ids'][i].unsqueeze(0),\n",
    "              'attention_mask': test_X['attention_mask'][i].unsqueeze(0)}\n",
    "\n",
    "    concept = test_y[i]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        meddra_code = CV.meddra_codes[the_model(encoded_phrase).argmax()]\n",
    "\n",
    "\n",
    "    model_answers.append(meddra_code)\n",
    "    #print('phrase: %s'%phrase)\n",
    "    #print('model: %s'%CV.meddra_code_to_meddra_term[meddra_code])\n",
    "\n",
    "\n",
    "\n",
    "    meddra_code = CV.meddra_codes[concept.argmax()]\n",
    "    real_answers.append(meddra_code)\n",
    "    #print(train_phrases[i])\n",
    "    #print('real: %s'%CV.meddra_code_to_meddra_term[meddra_code])\n",
    "    \n",
    "#print(classification_report(real_answers, model_answers, zero_division=0))\n",
    "f1_score(real_answers, model_answers, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-heavy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest_torch",
   "language": "python",
   "name": "latest_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
