{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "czech-lodge",
   "metadata": {},
   "source": [
    "<h3>Векторизуем словарь Meddra</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interior-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorization import ConceptVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "offshore-circular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "CV = ConceptVectorizer('cointegrated/rubert-tiny2', '../../Data/External/pt_rus.asc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outstanding-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting concept embeddings in cls_token mode...\n"
     ]
    }
   ],
   "source": [
    "CV.fit_transform(mode='cls_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "distinct-assault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0047,  0.6057,  0.8048,  ...,  0.4303,  1.0715, -0.4888],\n",
       "        [ 0.5371, -0.3121,  0.1231,  ...,  0.5167,  0.4018, -0.3266],\n",
       "        [ 0.2913, -0.5215,  0.4901,  ...,  0.9543,  0.4253, -0.1851],\n",
       "        ...,\n",
       "        [ 0.0108,  0.0136,  0.9450,  ...,  0.1267,  1.0630, -0.8467],\n",
       "        [ 0.1349, -0.0723,  0.2402,  ...,  0.2063,  0.3276, -0.7158],\n",
       "        [ 0.2104, -0.1122,  0.4180,  ...,  0.0780,  0.4703, -0.8100]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.thesaurus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "optimum-necklace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23954\n"
     ]
    }
   ],
   "source": [
    "print(len(CV.thesaurus_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "convertible-return",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cls_token'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.vectorization_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-commitment",
   "metadata": {},
   "source": [
    "<h3>Создадим датасет RDR</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affected-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "naughty-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "with jsonlines.open('../../Data/Raw/medNorm_14012022.jsonlines') as reader:\n",
    "    for obj in reader:\n",
    "        ds.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "julian-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(ds, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acknowledged-thomson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего фраз в трейне: 3476\n",
      "Всего фраз в тесте: 1750\n",
      "Уникальных фраз в трейне: 1204\n",
      "Уникальных фраз в тесте: 750\n",
      "162 концептов не входящих либо в трейн, либо в тест\n",
      "53 концептов, которые есть в тесте, но нет в трейне\n",
      "109 концептов, которые есть в трейне, но нет в тесте\n"
     ]
    }
   ],
   "source": [
    "#выцепим фразы с нормализацией по Meddra без их контекста\n",
    "\n",
    "train_phrases = []\n",
    "train_concepts = []\n",
    "\n",
    "test_phrases = []\n",
    "test_concepts = []\n",
    "\n",
    "for review in X_train:\n",
    "    for ent in review['objects']['MedEntity']:\n",
    "        if 'MedDRA' in ent.keys() and ent['MedDRA']!='':\n",
    "            #try:\n",
    "            train_concepts.append(CV.meddra_term_to_meddra_code[ent['MedDRA'].split('|')[0]])\n",
    "            #except:\n",
    "            #markup_errors+=1\n",
    "            #continue\n",
    "            train_phrases.append(ent['text'])\n",
    "            \n",
    "            \n",
    "for review in X_test:\n",
    "    for ent in review['objects']['MedEntity']:\n",
    "        if 'MedDRA' in ent.keys() and ent['MedDRA']!='':\n",
    "            #try:\n",
    "            test_concepts.append(CV.meddra_term_to_meddra_code[ent['MedDRA'].split('|')[0]])\n",
    "            #except:\n",
    "            #    markup_errors+=1\n",
    "            #    continue\n",
    "            test_phrases.append(ent['text'])\n",
    "            \n",
    "print('Всего фраз в трейне: %s'%len(train_phrases))\n",
    "print('Всего фраз в тесте: %s'%len(test_phrases))\n",
    "\n",
    "print('Уникальных фраз в трейне: %s'%len(set(train_phrases)))\n",
    "print('Уникальных фраз в тесте: %s'%len(set(test_phrases)))\n",
    "\n",
    "#Посмотрим на статистику разбиения\n",
    "print('%s концептов не входящих либо в трейн, либо в тест'%len(set.union(set(train_concepts), set(test_concepts)) - set.intersection(set(test_concepts), set(train_concepts))))\n",
    "print('%s концептов, которые есть в тесте, но нет в трейне'%len(set(test_concepts) - set(train_concepts)))\n",
    "print('%s концептов, которые есть в трейне, но нет в тесте'%len(set(train_concepts) - set(test_concepts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "working-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MedNormDataset\n",
    "\n",
    "RDR_train = MedNormDataset(train_phrases, train_concepts, CV, use_cuda=True)\n",
    "RDR_test = MedNormDataset(test_phrases, test_concepts, CV, use_cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-smart",
   "metadata": {},
   "source": [
    "<h2>Сама модель</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "important-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing Net: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing Net from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Net from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net loaded\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "#Динамический импорт класса нужной PreTrained модели по автоконфигурации\n",
    "from transformers import AutoConfig\n",
    "\n",
    "#Все, что нужно указать\n",
    "model_path = 'cointegrated/rubert-tiny2'\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(model_path)\n",
    "ConfigModelClass = cfg.__class__\n",
    "PreTrainedModelClassName = ConfigModelClass.__name__.replace('Config', 'PreTrainedModel')\n",
    "ModelClassName = cfg.__class__.__name__.replace('Config', 'Model')\n",
    "exec(\"from transformers import %s as PreTrainedModelClass\"%PreTrainedModelClassName)\n",
    "exec(\"from transformers import %s as ModelClass\"%ModelClassName)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "\n",
    "def get_cls_token_emb(model_output):\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    #embeddings = torch.nn.functional.normalize(embeddings, dim=1)\n",
    "    return embeddings\n",
    "\n",
    "normalized_embs = CV.thesaurus_embeddings.norm(dim=1)[:, None]\n",
    "normalized_embs = CV.thesaurus_embeddings / torch.clamp(normalized_embs, min=1e-8)\n",
    "normalized_embs = normalized_embs.transpose(0, 1)\n",
    "normalized_embs = normalized_embs.to(device)\n",
    "\n",
    "class Net(PreTrainedModelClass):\n",
    "    def __init__(self, config: ConfigModelClass):\n",
    "        super(Net, self).__init__(config)\n",
    "        self.bert = ModelClass(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.bert(**x)\n",
    "        #with torch.no_grad():\n",
    "        x = get_cls_token_emb(emb)\n",
    "        #имеем две матрицы x - (batch_size, emb_size) и thesaurus_embeddings - (thesaurus_size, emb_size)\n",
    "        #надо посчитать косинусную близость близость между каждым вектором x и каждым вложением из тезауруса\n",
    "        #решение: https://stackoverflow.com/questions/50411191/how-to-compute-the-cosine-similarity-in-pytorch-for-all-rows-in-a-matrix-with-re\n",
    "        x_n = x.norm(dim=1)[:, None] \n",
    "        x_n = x / torch.clamp(x_n, min=1e-8)\n",
    "        #b_norm = CV.thesaurus_embeddings / torch.clamp(b_, min=1e-9)\n",
    "        cos_sim = torch.mm(x_n, normalized_embs)\n",
    "        x = F.softmax(cos_sim, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net.from_pretrained('cointegrated/rubert-tiny2', config=cfg)\n",
    "net.to(device)\n",
    "print('Net loaded')\n",
    "\n",
    "#net2 = Net.from_pretrained('cointegrated/rubert-tiny2', config=cfg)\n",
    "#net2.to(device)\n",
    "#print('Net loaded')\n",
    "#normalized_embs.to(device)\n",
    "\n",
    "#for param in net.parameters():\n",
    "#    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-heart",
   "metadata": {},
   "source": [
    "<h2>Обучение модели</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "needed-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters())\n",
    "#optimizer2 = optim.AdamW(net2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "congressional-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-engagement",
   "metadata": {},
   "source": [
    "Для большей детерменированности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ongoing-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n",
    "torch.use_deterministic_algorithms(mode=False)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "gorgeous-authorization",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1738/1738 [00:42<00:00, 40.98batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "batch_size=2\n",
    "trainloader = torch.utils.data.DataLoader(RDR_train, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "net.train()\n",
    "for epoch in range(1, 2):\n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = data['tokenized_phrases']\n",
    "            labels = data['one_hot_labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if device=='cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            #чтобы посчитать accuracy нужно конвертнуть в one-hot\n",
    "            \n",
    "            #max_idx = torch.argmax(outputs, 1, keepdim=True)\n",
    "            #one_hot = torch.FloatTensor(outputs.shape).to(device)\n",
    "            #one_hot.zero_()\n",
    "            #one_hot.scatter_(1, max_idx, 1)\n",
    "            #correct = torch.all(torch.eq(labels, one_hot),  dim=1).sum().item()\n",
    "\n",
    "            #correct = (one_hot == labels)\n",
    "            #accuracy = correct / batch_size\n",
    "            #tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-qualification",
   "metadata": {},
   "source": [
    "<h2>Тест модели с cls_token</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "functional-agency",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [00:07<00:00, 242.88batch/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4257142857142857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "net.eval()\n",
    "\n",
    "model_answers=[]\n",
    "real_answers=[]\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(RDR_test, batch_size=1,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "with tqdm(testloader, unit=\"batch\") as eval_process:\n",
    "    for data in eval_process:\n",
    "\n",
    "        #tepoch.set_description(f\"Progress\")\n",
    "\n",
    "        inputs = data['tokenized_phrases']\n",
    "        labels = data['one_hot_labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_meddra_code = CV.meddra_codes[net(inputs).argmax()]\n",
    "\n",
    "\n",
    "        model_answers.append(pred_meddra_code)\n",
    "        real_answers.append(data['label_codes'])\n",
    "\n",
    "f1_score(real_answers, model_answers, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-shape",
   "metadata": {},
   "source": [
    "<h2>Инференс</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "psychological-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase: температурой 37,8\n",
      "model: Ринорея\n",
      "real: Пирексия\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "i = randint(1, len(RDR_test))\n",
    "\n",
    "net.eval()\n",
    "\n",
    "phrase = {k: tensor.unsqueeze(0) for k, tensor in RDR_test[i]['tokenized_phrases'].items()}\n",
    "concept = RDR_test[i]['label_codes']\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_answer = CV.meddra_codes[net(phrase).argmax()]\n",
    "    \n",
    "\n",
    "\n",
    "print('phrase: %s'%RDR_test[i]['phrases'])\n",
    "print('model: %s'%CV.meddra_code_to_meddra_term[model_answer])\n",
    "\n",
    "\n",
    "\n",
    "print('real: %s'%RDR_test[i]['label_terms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-burden",
   "metadata": {},
   "source": [
    "<h2>Сохранение и загрузка модели</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "statewide-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './cadec_SoTa_on_RDR_rubert_cls_tok.pt')\n",
    "torch.save(optimizer.state_dict(), './cadec_SoTa_on_RDR_rubert_cls_tok_opt.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest_torch",
   "language": "python",
   "name": "latest_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
