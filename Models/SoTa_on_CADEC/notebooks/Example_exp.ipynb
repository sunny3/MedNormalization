{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "taken-surrey",
   "metadata": {},
   "source": [
    "<h3>Глобальные переменные</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "associate-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#путь до языковой модели, которая будет учиться в составе сетки. Полный список в https://huggingface.co/models\n",
    "transformer_model_path = 'DeepPavlov/rubert-base-cased'\n",
    "#Использовать ли видеокарты. torch может автоматически определить эту переменную\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-draft",
   "metadata": {},
   "source": [
    "<h3>Векторизуем словарь Meddra</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reported-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorization import ConceptVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amended-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сюда прописывается путь до модели и путь до файла словаря расширения .asc, который необходимо векторизовать\n",
    "#для того, чтобы использовать векторизатор без векторизационной модели, \n",
    "#например, когда готовые векторы концептов уже есть, и модель использовать не нужно, ставьте use_model=False\n",
    "CV = ConceptVectorizer(transformer_model_path, '../../Data/External/pt_rus.asc', \\\n",
    "                             use_concept_less=False, use_model=True, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-baseball",
   "metadata": {},
   "source": [
    "Расчет вложений словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "swiss-banner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting concept embeddings in mean_pooling mode...\n",
      "Compute embeddings...\n",
      "in cpu mode there is no progress bar\n",
      "Embedding aggregation...\n",
      "Concept embeddings have computed in 251.15103459358215 seconds\n"
     ]
    }
   ],
   "source": [
    "#используется только для векторизации словаря\n",
    "#когда вектора готовы, проще их загрузить ячейкой с загрузкой\n",
    "CV.fit_transform(mode='mean_pooling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-toronto",
   "metadata": {},
   "source": [
    "Сохранение рассчитанных векторов концептов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reduced-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CV.thesaurus_embeddings, './Model_weights/rubert_thesaurus_embeddings_meddra_origin.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-hometown",
   "metadata": {},
   "source": [
    "Загрузка готовых вложений (если они есть). Загружать вложения после fit_transform() не надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adjusted-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV.thesaurus_embeddings = torch.load('./Model_weights/rubert_thesaurus_embeddings_meddra_origin.pt') \n",
    "CV.normalization_mode = 'mean_pooling'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-group",
   "metadata": {},
   "source": [
    "<h3>Обработка датасета, пример с RDRS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contemporary-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intelligent-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "with jsonlines.open('../../Data/Raw/medNorm_16022022.jsonlines') as reader:\n",
    "    for obj in reader:\n",
    "        ds.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intellectual-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(ds, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "insured-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "#выцепим фразы с нормализацией по Meddra без их контекста\n",
    "train_phrases = []\n",
    "train_concepts = []\n",
    "\n",
    "test_phrases = []\n",
    "test_concepts = []\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "\n",
    "USE_CONCEPT_LESS=False\n",
    "\n",
    "for review in X_train:\n",
    "    for ent in review['objects']['MedEntity']:\n",
    "        if 'MedDRA' in ent.keys():\n",
    "            if ent['MedDRA']=='':\n",
    "                if USE_CONCEPT_LESS:\n",
    "                    ent['MedDRA'] = 'CONCEPT_LESS'\n",
    "                else:\n",
    "                    continue\n",
    "            train_concepts.append(CV.meddra_term_to_meddra_code[ent['MedDRA'].split('|')[0]])\n",
    "            train_phrases.append(ent['text'])\n",
    "            train_ids.append(review['meta']['fileName'])\n",
    "            \n",
    "for review in X_test:\n",
    "    for ent in review['objects']['MedEntity']:\n",
    "        if 'MedDRA' in ent.keys():\n",
    "            if ent['MedDRA']=='':\n",
    "                if USE_CONCEPT_LESS:\n",
    "                    ent['MedDRA'] = 'CONCEPT_LESS'\n",
    "                else:\n",
    "                    continue\n",
    "            test_concepts.append(CV.meddra_term_to_meddra_code[ent['MedDRA'].split('|')[0]])\n",
    "            test_phrases.append(ent['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fewer-morris",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from dataset import MedNormDataset\n",
    "\n",
    "ds_train = MedNormDataset(train_phrases, train_concepts, CV, use_cuda=USE_CUDA)\n",
    "ds_test = MedNormDataset(test_phrases, test_concepts, CV, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-traveler",
   "metadata": {},
   "source": [
    "<h2>Обучение</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-brass",
   "metadata": {},
   "source": [
    "<h4>Настройка глобальных переменных для большей детерменированности</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pregnant-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-refrigerator",
   "metadata": {},
   "source": [
    "<h4>Импорт модели</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "binding-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net loaded\n"
     ]
    }
   ],
   "source": [
    "from models import CADEC_SoTa\n",
    "\n",
    "\n",
    "net = CADEC_SoTa(transformer_model_path, CV.thesaurus_embeddings)\n",
    "#перемещение модели на вычислительные мощности, либо cpu, либо cuda\n",
    "device = 'cuda' if USE_CUDA else 'cpu'\n",
    "net.to(device)\n",
    "print('Net loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-basket",
   "metadata": {},
   "source": [
    "<h4>Инициализация доп. объектов для обучения</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "copyrighted-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from evaluator import Evaluator\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.0001)\n",
    "#scheduler = CyclicLR(optimizer, base_lr=4.7e-05, max_lr=0.0002, cycle_momentum=False) если с циклическим lr\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "evaluator = Evaluator(train_concepts, test_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-track",
   "metadata": {},
   "source": [
    "<h3>Процесс обучения и рассчета точностей</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "#Гиперпараметры\n",
    "batch_size=16\n",
    "epochs = 1\n",
    "#номер эксперимента\n",
    "exp_num = 11\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(ds_test, batch_size=1, shuffle=False)\n",
    "\n",
    "net.train()\n",
    "initial_loss = None\n",
    "for epoch in range(1, epochs+1):\n",
    "    #обучение модели в эпохе\n",
    "    net.train()\n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = data['tokenized_phrases']\n",
    "            labels = data['one_hot_labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if USE_CUDA:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = net(inputs)['output']\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = net(inputs)['output']\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if initial_loss is None:\n",
    "                initial_loss = loss.item()\n",
    "            tepoch.set_postfix(loss_decrease = str(initial_loss/loss.item()))\n",
    "    #расчет точностей модели после эпохи\n",
    "    net.eval()\n",
    "    model_answers=[]\n",
    "    real_answers=[]\n",
    "    with tqdm(testloader, unit=\"batch\") as eval_process:\n",
    "        for data in eval_process:\n",
    "\n",
    "            inputs = data['tokenized_phrases']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs_dict = net(inputs)\n",
    "                pred_meddra_code = CV.meddra_codes[outputs_dict['output'].argmax()]\n",
    "\n",
    "\n",
    "            model_answers.append(pred_meddra_code)\n",
    "            real_answers.append(data['label_codes'])\n",
    "    \n",
    "    evaluator.get_all_f1_scores(real_answers, model_answers)\n",
    "            \n",
    "print('Finished Training')\n",
    "torch.save(net,'./Model_weights/rubert_exp_%s.pt'%exp_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-importance",
   "metadata": {},
   "source": [
    "Инференс (подать фразу в сетку и посмотреть, какой концепт она выдаст)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-production",
   "metadata": {},
   "source": [
    "Чтобы сетка выдавала отсутствие концепта на несвязанные с медрой фразы, раскомментируйте строчку с label_concepless_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "requested-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase: Болезнь\n",
      "model output: Повышенная температура тела\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "phrase = 'Болезнь'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_path)\n",
    "encoded_input = tokenizer([phrase], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_dict = net(inputs)\n",
    "    #outputs_dict.label_concepless_tensors(score_treshold = 6.1977e-05)\n",
    "    pred_meddra_code = CV.meddra_codes[outputs_dict['output'].argmax()]\n",
    "    \n",
    "print('phrase: %s'%phrase)\n",
    "print('model output: %s'%CV.meddra_code_to_meddra_term[pred_meddra_code])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-stock",
   "metadata": {},
   "source": [
    "<h2>Сохранение и загрузка модели</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-mitchell",
   "metadata": {},
   "source": [
    "сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "coupled-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './trained_model_weights.pt')\n",
    "torch.save(optimizer.state_dict(), './trained_model_opt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-entry",
   "metadata": {},
   "source": [
    "загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "agricultural-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = torch.load('./trained_model_weights.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normalization env",
   "language": "python",
   "name": "latest_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
