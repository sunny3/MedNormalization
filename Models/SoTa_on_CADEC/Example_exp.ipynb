{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "charming-virginia",
   "metadata": {},
   "source": [
    "<h3>Глобальные переменные</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quantitative-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#путь до языковой модели, которая будет учиться в составе сетки. Полный список в https://huggingface.co/models\n",
    "transformer_model_path = './bert-base-cased'\n",
    "\n",
    "#Использовать ли видеокарты. torch может автоматически определить эту переменную\n",
    "#Для этого расскоментируйте строки\n",
    "#import torch\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-income",
   "metadata": {},
   "source": [
    "<h3>Векторизуем словарь Meddra</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worst-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorization import ConceptVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impressive-perth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "corporate-passenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#сюда прописывается путь до модели и путь до файла словаря расширения .asc, который необходимо векторизовать\n",
    "CV = ConceptVectorizer(transformer_model_path, '../../Data/External/pt_en.asc', \\\n",
    "                             use_concept_less=False, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-bacon",
   "metadata": {},
   "source": [
    "Расчет вложений словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "leading-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting concept embeddings in mean_pooling mode...\n",
      "Compute embeddings...\n",
      "in cpu mode there is no progress bar\n",
      "Embedding aggregation...\n",
      "Concept embeddings have computed in 251.15103459358215 seconds\n"
     ]
    }
   ],
   "source": [
    "#используется только для векторизации словаря\n",
    "#когда вектора готовы, проще их загрузить ячейкой с загрузкой\n",
    "CV.fit_transform(mode='mean_pooling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-luther",
   "metadata": {},
   "source": [
    "Сохранение рассчитанных векторов концептов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "administrative-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(CV.thesaurus_embeddings, 'model_thesaurus_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-testament",
   "metadata": {},
   "source": [
    "Загрузка готовых вложений (если они есть). Загружать вложения после fit_transform() не надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "CV.thesaurus_embeddings = torch.load('model_thesaurus_embeddings.pt')\n",
    "CV.normalization_mode = 'mean_pooling'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-shark",
   "metadata": {},
   "source": [
    "<h3>Обработка датасета, пример с CADEC</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "voluntary-concentrate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docName</th>\n",
       "      <th>term</th>\n",
       "      <th>meddra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIPITOR.877</td>\n",
       "      <td>MUSCLES ACHED</td>\n",
       "      <td>10028411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIPITOR.877</td>\n",
       "      <td>DIFFICULTY IN WALKING</td>\n",
       "      <td>10017577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIPITOR.877</td>\n",
       "      <td>AFFECTED MY BALANCE</td>\n",
       "      <td>10027175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIPITOR.169</td>\n",
       "      <td>gas</td>\n",
       "      <td>10016766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIPITOR.169</td>\n",
       "      <td>loose stools</td>\n",
       "      <td>10012735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6313</th>\n",
       "      <td>LIPITOR.346</td>\n",
       "      <td>tremendous neck pain</td>\n",
       "      <td>10028836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6314</th>\n",
       "      <td>LIPITOR.346</td>\n",
       "      <td>severe headaches</td>\n",
       "      <td>10019211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6315</th>\n",
       "      <td>LIPITOR.346</td>\n",
       "      <td>rash</td>\n",
       "      <td>10037844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>LIPITOR.346</td>\n",
       "      <td>hemorrhoids</td>\n",
       "      <td>10019022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>LIPITOR.346</td>\n",
       "      <td>weakness in my legs</td>\n",
       "      <td>10028372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6318 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          docName                   term    meddra\n",
       "0     LIPITOR.877          MUSCLES ACHED  10028411\n",
       "1     LIPITOR.877  DIFFICULTY IN WALKING  10017577\n",
       "2     LIPITOR.877    AFFECTED MY BALANCE  10027175\n",
       "3     LIPITOR.169                    gas  10016766\n",
       "4     LIPITOR.169           loose stools  10012735\n",
       "...           ...                    ...       ...\n",
       "6313  LIPITOR.346   tremendous neck pain  10028836\n",
       "6314  LIPITOR.346       severe headaches  10019211\n",
       "6315  LIPITOR.346                   rash  10037844\n",
       "6316  LIPITOR.346            hemorrhoids  10019022\n",
       "6317  LIPITOR.346    weakness in my legs  10028372\n",
       "\n",
       "[6318 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dir = '../../Data/Raw/cadec_table_grartem.csv'\n",
    "\n",
    "df = pd.read_csv(data_dir)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-heather",
   "metadata": {},
   "source": [
    "Поле **term** в _df_ содержит фразу, которую необходимо нормализовать<br>\n",
    "Поле **meddra** в _df_ код концепта в словаре meddra<br>\n",
    "<br>\n",
    "В случае, когда у фразы несколько кодов meddra, они записаны через символ '|' в колонке **meddra**, например '100105|100106'<br>\n",
    "В случае, когда у фразы НЕТ концепта, в колонке **meddra** у нее стоит 'CONCEPT_LESS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-quantum",
   "metadata": {},
   "source": [
    "<h4>Тут нужно дописать часть обработки данных</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-rehabilitation",
   "metadata": {},
   "source": [
    "а именно:\n",
    "- Отфильтровать строки со значением CONCEPT_LESS\n",
    "- У всех фраз, у которых несколько кодов meddra, оставить только один код\n",
    "- Отфильтровать фразы у которых кода концепта нет в списке CV.meddra_codes\n",
    "- Разделить набор данных на train и test часть. Фразы в списках train_phrases / test_phrases, соотвествующие им концепты в списках train_concepts / test_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ячейка с кодом обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mighty-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MedNormDataset\n",
    "\n",
    "ds_train = MedNormDataset(train_phrases, train_concepts, CV, use_cuda=use_cuda)\n",
    "ds_test = MedNormDataset(test_phrases, test_concepts, CV, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-scratch",
   "metadata": {},
   "source": [
    "<h2>Импорт модели</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "single-discharge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net loaded\n"
     ]
    }
   ],
   "source": [
    "from models import CADEC_SoTa\n",
    "\n",
    "\n",
    "net = CADEC_SoTa(transformer_model_path, CV.thesaurus_embeddings)\n",
    "#перемещение модели на вычислительные мощности, либо cpu, либо cuda\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "net.to(device)\n",
    "print('Net loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-enlargement",
   "metadata": {},
   "source": [
    "<h3>Инициализация доп. объектов для обучения</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "electric-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "featured-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-strengthening",
   "metadata": {},
   "source": [
    "Настройка глобальных переменных для большей детерменированности <br>\n",
    "Это нужно, чтобы одна и та же сетка сходилась к одним и тем же значениям при одних и тех же параметрах для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "balanced-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":16:8\"\n",
    "torch.use_deterministic_algorithms(mode=False)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-story",
   "metadata": {},
   "source": [
    "<h3>Процесс обучения и рассчета точностей</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "crazy-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 250/250 [07:23<00:00,  1.78s/batch, loss_decrease=1.0000001882925529]\n",
      "100%|██████████| 1970/1970 [01:45<00:00, 18.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5106598984771573\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "#Гиперпараметры\n",
    "batch_size=16\n",
    "epochs = 1\n",
    "    \n",
    "trainloader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(ds_test, batch_size=1, shuffle=False)\n",
    "\n",
    "net.train()\n",
    "initial_loss = None\n",
    "for epoch in range(1, epochs+1):\n",
    "    #обучение модели в эпохе\n",
    "    net.train()\n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            inputs = data['tokenized_phrases']\n",
    "            labels = data['one_hot_labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = net(inputs)['output']\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = net(inputs)['output']\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if initial_loss is None:\n",
    "                initial_loss = loss.item()\n",
    "            tepoch.set_postfix(loss_decrease = str(initial_loss/loss.item()))\n",
    "    #расчет точностей модели после эпохи\n",
    "    net.eval()\n",
    "    model_answers=[]\n",
    "    real_answers=[]\n",
    "    with tqdm(testloader, unit=\"batch\") as eval_process:\n",
    "        for data in eval_process:\n",
    "\n",
    "            inputs = data['tokenized_phrases']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs_dict = net(inputs)\n",
    "                pred_meddra_code = CV.meddra_codes[outputs_dict['output'].argmax()]\n",
    "\n",
    "\n",
    "            model_answers.append(pred_meddra_code)\n",
    "            real_answers.append(data['label_codes'])\n",
    "    \n",
    "    print(f1_score(real_answers, model_answers, average='micro'))\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-handling",
   "metadata": {},
   "source": [
    "Инференс (подать фразу в сетку и посмотреть, какой концепт она выдаст)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-sullivan",
   "metadata": {},
   "source": [
    "Чтобы сетка выдавала отсутствие концепта на несвязанные с медрой фразы, раскомментируйте строчку с label_concepless_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "coordinate-produce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase: Disease\n",
      "model output: Physical disability\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "phrase = 'Disease'\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_model_path)\n",
    "encoded_input = tokenizer([phrase], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_dict = net(inputs)\n",
    "    #outputs_dict.label_concepless_tensors(score_treshold = 6.1977e-05)\n",
    "    pred_meddra_code = CV.meddra_codes[outputs_dict['output'].argmax()]\n",
    "    \n",
    "print('phrase: %s'%phrase)\n",
    "print('model output: %s'%CV.meddra_code_to_meddra_term[pred_meddra_code])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-excellence",
   "metadata": {},
   "source": [
    "<h2>Сохранение и загрузка модели</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-adoption",
   "metadata": {},
   "source": [
    "сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "smoking-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './trained_model_weights.pt')\n",
    "torch.save(optimizer.state_dict(), './trained_model_opt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-carolina",
   "metadata": {},
   "source": [
    "загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "liberal-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = torch.load('./trained_model_weights.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normalization env",
   "language": "python",
   "name": "latest_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
